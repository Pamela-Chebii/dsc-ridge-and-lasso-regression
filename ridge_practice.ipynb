{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "\n",
    "data = pd.read_csv('auto-mpg.csv') \n",
    "\n",
    "y = data[['mpg']]\n",
    "X = data.drop(['mpg', 'car name', 'origin'], axis=1)\n",
    "\n",
    "# Perform test train split\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the data\n",
    "scaler = MinMaxScaler()\n",
    "x_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a ridge, lasso and regular linear regression model  \n",
    "# Note that in scikit-learn, the regularization parameter is denoted by alpha (and not lambda)\n",
    "ridge = Ridge(alpha=0.5)\n",
    "ridge.fit(x_train_scaled, y_train)\n",
    "\n",
    "lasso = Lasso(alpha=0.5)\n",
    "lasso.fit(x_train_scaled, y_train)\n",
    "\n",
    "lin = LinearRegression()\n",
    "lin.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictors for both training and test sets\n",
    "\n",
    "y_pred_ridge_train = ridge.predict(x_train_scaled)\n",
    "y_pred_ridge_test = ridge.predict(X_test_scaled)\n",
    "\n",
    "y_pred_lasso_train = lasso.predict(x_train_scaled)\n",
    "y_pred_lasso_test = lasso.predict(X_test_scaled)\n",
    "\n",
    "y_pred_lin_train = lin.predict(x_train_scaled)\n",
    "y_pred_lin_test = lin.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error Ridge Train 9.79807951552983\n",
      "Mean Squared Error Ridge Test 17.523692433834455\n",
      "\n",
      "\n",
      "Mean Squared Error Lasso Train 16.24445079708179\n",
      "Mean Squared Error Lasso Test 30.03463631503097\n",
      "\n",
      "\n",
      "Mean Squared Error Linear Train 9.700888480581275\n",
      "Mean Squared Error Linear Test 16.748025313964717\n"
     ]
    }
   ],
   "source": [
    "# MSE for train and test sets for each of the three models\n",
    "\n",
    "print(\"Mean Squared Error Ridge Train\", mean_squared_error(y_train, y_pred_ridge_train))\n",
    "print(\"Mean Squared Error Ridge Test\", mean_squared_error(y_test, y_pred_ridge_test))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Mean Squared Error Lasso Train\", mean_squared_error(y_train, y_pred_lasso_train))\n",
    "print(\"Mean Squared Error Lasso Test\", mean_squared_error(y_test, y_pred_lasso_test))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Mean Squared Error Linear Train\", mean_squared_error(y_train, y_pred_lin_train))\n",
    "print(\"Mean Squared Error Linear Test\", mean_squared_error(y_test, y_pred_lin_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Parameter Coefficients [[ -2.06904445  -2.88593443  -1.81801505 -15.23785349  -1.45594148\n",
      "    8.1440177 ]]\n",
      "Lasso Parameter Coefficients [-9.09743525 -0.         -0.         -4.02703963  0.          3.92348219]\n",
      "Linear Model Parameter Coefficients [[ -1.33790698  -1.05300843  -0.08661412 -19.26724989  -0.37043697\n",
      "    8.56051229]]\n"
     ]
    }
   ],
   "source": [
    "# Let's see how including ridge and lasso changed our parameter estimates.\n",
    "print(\"Ridge Parameter Coefficients\", ridge.coef_)\n",
    "print(\"Lasso Parameter Coefficients\", lasso.coef_)\n",
    "print(\"Linear Model Parameter Coefficients\", lin.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularized Polynomial Regression vs. Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error Polynomial Ridge Model 9.79807951552983\n",
      "Test Error Polynomial Ridge Model 10.705099905649627\n",
      "\n",
      "\n",
      "Train Error Polynomial Lasso Model 16.42963282609318\n",
      "Test Error Polynomial Lasso Model 30.384937999587358\n",
      "\n",
      "\n",
      "Train Error Unpenalized Polynomial Model 2.6722252011967364e-18\n",
      "Test Error Unpenalized Polynomial Model 184300.8258927713\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "poly = PolynomialFeatures(degree=6)\n",
    "X_train_poly =  poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Standardze the data\n",
    "X_train_transformed = scaler.fit_transform(X_train_poly)\n",
    "X_test_transformed = scaler.transform(X_test_poly)\n",
    "\n",
    "# Fit Models\n",
    "ridge.fit(X_train_transformed, y_train)\n",
    "lasso.fit(X_train_transformed, y_train)\n",
    "lin.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "y_predict_ridge_train = ridge.predict(X_train_transformed)\n",
    "y_pred_ridge_test = ridge.predict(X_test_transformed)\n",
    "\n",
    "y_pred_lasso_train = lasso.predict(X_train_transformed)\n",
    "y_pred_lasso_test = lasso.predict(X_test_transformed)\n",
    "\n",
    "y_pred_lin_train = lin.predict(X_train_transformed)\n",
    "y_pred_lin_test = lin.predict(X_test_transformed)\n",
    "\n",
    "\n",
    "# Display Results\n",
    "print('Train Error Polynomial Ridge Model', mean_squared_error(y_train, y_pred_ridge_train))\n",
    "print('Test Error Polynomial Ridge Model', mean_squared_error(y_test, y_pred_ridge_test))\n",
    "print('\\n')\n",
    "print('Train Error Polynomial Lasso Model', mean_squared_error(y_train, y_pred_lasso_train))\n",
    "print('Test Error Polynomial Lasso Model', mean_squared_error(y_test, y_pred_lasso_test))\n",
    "print('\\n')\n",
    "print('Train Error Unpenalized Polynomial Model', mean_squared_error(y_train, y_pred_lin_train))\n",
    "print('Test Error Unpenalized Polynomial Model', mean_squared_error(y_test, y_pred_lin_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- The model that performed best is the combination of ridge and polynomial. It is able to balance between bias and variance. \n",
    "- The unpenalized model is overfitting ie the variance is very high"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
